#!/usr/bin/env node

/**
 * PEÅNY IMPORT historycznych danych hydrologicznych
 * Import wszystkich ~608,653 pomiarÃ³w z pliku deximstr_hydro.sql
 */

const fs = require('fs');
const path = require('path');
const { PrismaClient } = require('@prisma/client');

const prisma = new PrismaClient();

console.log('ğŸš€ HYDRO API - PEÅNY IMPORT HISTORYCZNYCH DANYCH');
console.log('=' * 80);

// Konfiguracja produkcyjna
const CONFIG = {
    batchSize: 1000,           // WiÄ™ksze partie dla wydajnoÅ›ci
    maxRetries: 5,
    delayBetweenBatches: 500,  // KrÃ³tsze opÃ³Åºnienia
    dryRun: false,
    logEveryNBatches: 10       // Loguj co 10 partii
};

// Åadowanie mapowania historycznych stacji
function loadHistoricalMapping() {
    const mappingFile = path.join(process.cwd(), 'historical-station-mapping.json');
    
    if (!fs.existsSync(mappingFile)) {
        throw new Error('Plik mapowania nie istnieje. Uruchom najpierw: node scripts/create-historical-mapping.js');
    }
    
    const mapping = JSON.parse(fs.readFileSync(mappingFile, 'utf8'));
    console.log(`ğŸ—ºï¸  ZaÅ‚adowano mapowanie dla ${Object.keys(mapping).length} historycznych stacji`);
    
    return mapping;
}

// Funkcja do parsowania peÅ‚nego pliku SQL
function parseSQLFileFull(filePath) {
    console.log('ğŸ“– Czytanie peÅ‚nego pliku SQL...');
    
    if (!fs.existsSync(filePath)) {
        throw new Error(`Plik nie istnieje: ${filePath}`);
    }

    const fileSize = (fs.statSync(filePath).size / 1024 / 1024).toFixed(2);
    console.log(`ğŸ“Š Rozmiar pliku: ${fileSize} MB`);
    console.log('â³ Czytanie caÅ‚ego pliku do pamiÄ™ci...');
    
    const content = fs.readFileSync(filePath, 'utf8');
    console.log('âœ… Plik zaÅ‚adowany do pamiÄ™ci');
    
    // WyodrÄ™bniamy dane INSERT dla measurements
    console.log('ğŸ” Szukanie INSERT statements dla measurements...');
    const insertRegex = /INSERT INTO `measurements`[^(]*\(([^)]+)\)\s+VALUES\s*([\s\S]*?);/gi;
    const data = {};
    
    let match;
    let totalRecords = 0;
    let insertCount = 0;
    
    while ((match = insertRegex.exec(content)) !== null) {
        insertCount++;
        const tableName = 'measurements';
        const columnNames = match[1].split(',').map(col => col.trim().replace(/`/g, ''));
        const valuesString = match[2];
        
        if (!data[tableName]) {
            data[tableName] = { columns: columnNames, rows: [] };
            console.log('ğŸ“Š Kolumny w tabeli measurements:');
            columnNames.forEach((col, index) => {
                console.log(`   ${index + 1}. ${col}`);
            });
        }
        
        // Parsujemy wartoÅ›ci
        const valueRegex = /\(([^)]+)\)/g;
        let valueMatch;
        let recordsInThisInsert = 0;
        
        while ((valueMatch = valueRegex.exec(valuesString)) !== null) {
            const values = parseValues(valueMatch[1]);
            data[tableName].rows.push(values);
            totalRecords++;
            recordsInThisInsert++;
        }
        
        if (insertCount % 100 === 0) {
            console.log(`ğŸ“¥ Przetworzono ${insertCount} INSERT statements, ${totalRecords} rekordÃ³w...`);
        }
    }
    
    console.log(`ğŸ“¥ Znaleziono ${insertCount} INSERT statements`);
    console.log(`ğŸ“Š ÅÄ…cznie rekordÃ³w: ${totalRecords}`);
    
    return { data };
}

// Funkcja do parsowania wartoÅ›ci SQL
function parseValues(valueString) {
    const values = [];
    let current = '';
    let inQuotes = false;
    let quoteChar = '';
    
    for (let i = 0; i < valueString.length; i++) {
        const char = valueString[i];
        
        if (!inQuotes && (char === "'" || char === '"')) {
            inQuotes = true;
            quoteChar = char;
        } else if (inQuotes && char === quoteChar) {
            if (valueString[i + 1] === quoteChar) {
                current += char;
                i++;
            } else {
                inQuotes = false;
                quoteChar = '';
            }
        } else if (!inQuotes && char === ',') {
            values.push(parseValue(current.trim()));
            current = '';
        } else {
            current += char;
        }
    }
    
    if (current.trim()) {
        values.push(parseValue(current.trim()));
    }
    
    return values;
}

function parseValue(value) {
    if (value === 'NULL') return null;
    if (value.startsWith("'") && value.endsWith("'")) {
        return value.slice(1, -1).replace(/''/g, "'");
    }
    if (value.startsWith('"') && value.endsWith('"')) {
        return value.slice(1, -1).replace(/""/g, '"');
    }
    if (/^\d+$/.test(value)) return parseInt(value);
    if (/^\d+\.\d+$/.test(value)) return parseFloat(value);
    return value;
}

// Funkcja mapowania pomiarÃ³w (produkcyjna wersja)
function mapMeasurementData(row, columns, historicalMapping) {
    const columnMap = {
        'station_id': 'stationId',
        'measurement_timestamp': 'measurementTimestamp',
        'water_level': 'waterLevel',
        'flow_rate': 'flowRate',
        'temperature': 'temperature',
        'source': 'source',
        'created_at': 'createdAt'
    };
    
    const mapped = {};
    columns.forEach((column, index) => {
        const prismaField = columnMap[column];
        if (prismaField && row[index] !== undefined) {
            let value = row[index];
            
            if (column.includes('timestamp') || column.includes('_at')) {
                value = value ? new Date(value) : new Date();
            } else if (['water_level'].includes(column)) {
                value = value ? parseInt(value) : null;
            } else if (['flow_rate', 'temperature'].includes(column)) {
                value = value ? parseFloat(value) : null;
            }
            
            mapped[prismaField] = value;
        }
    });
    
    if (!mapped.source) {
        mapped.source = 'historical';
    }
    
    // Mapowanie historical_station_id â†’ UUID
    if (mapped.stationId) {
        const historicalStationId = mapped.stationId.toString();
        const mappedUuid = historicalMapping[historicalStationId];
        
        if (mappedUuid) {
            mapped.stationId = mappedUuid;
        } else {
            return null; // Pomijamy pomiary dla nieznanych stacji
        }
    }
    
    return mapped;
}

// Funkcja do peÅ‚nego importu w partiach
async function importMeasurementsFull(data, historicalMapping) {
    const { columns, rows } = data;
    const totalRows = rows.length;
    
    console.log(`\nğŸš€ ROZPOCZYNAM PEÅNY IMPORT: ${totalRows.toLocaleString()} rekordÃ³w`);
    console.log(`ğŸ“¦ Rozmiar partii: ${CONFIG.batchSize}`);
    console.log(`â±ï¸  Szacowany czas: ${Math.ceil(totalRows / CONFIG.batchSize * CONFIG.delayBetweenBatches / 1000 / 60)} minut\n`);
    
    let imported = 0;
    let failed = 0;
    let skipped = 0;
    const stationStats = {};
    const startTime = Date.now();
    
    const totalBatches = Math.ceil(totalRows / CONFIG.batchSize);
    
    for (let i = 0; i < totalRows; i += CONFIG.batchSize) {
        const batchNumber = Math.floor(i / CONFIG.batchSize) + 1;
        const batch = rows.slice(i, i + CONFIG.batchSize);
        const mappedBatch = [];
        
        // Mapujemy kaÅ¼dy rekord
        for (const row of batch) {
            const mapped = mapMeasurementData(row, columns, historicalMapping);
            if (mapped) {
                mappedBatch.push(mapped);
                
                // Statystyki stacji
                const stationId = mapped.stationId;
                stationStats[stationId] = (stationStats[stationId] || 0) + 1;
            } else {
                skipped++;
            }
        }
        
        if (mappedBatch.length === 0) {
            if (batchNumber % CONFIG.logEveryNBatches === 0) {
                console.log(`âš ï¸  Partia ${batchNumber}/${totalBatches}: Wszystkie rekordy pominiÄ™te`);
            }
            continue;
        }
        
        let retryCount = 0;
        let success = false;
        
        while (retryCount < CONFIG.maxRetries && !success) {
            try {
                await prisma.measurement.createMany({
                    data: mappedBatch,
                    skipDuplicates: true
                });
                
                imported += mappedBatch.length;
                success = true;
                
                // Loguj co N partii lub na koÅ„cu
                if (batchNumber % CONFIG.logEveryNBatches === 0 || batchNumber === totalBatches) {
                    const progress = ((batchNumber / totalBatches) * 100).toFixed(1);
                    const elapsed = (Date.now() - startTime) / 1000 / 60;
                    const eta = elapsed / (batchNumber / totalBatches) - elapsed;
                    
                    console.log(`âœ… Partia ${batchNumber}/${totalBatches} (${progress}%) - ${mappedBatch.length} rekordÃ³w | ETA: ${eta.toFixed(1)}min`);
                }
                
            } catch (error) {
                retryCount++;
                console.error(`âŒ BÅ‚Ä…d w partii ${batchNumber} (prÃ³ba ${retryCount}/${CONFIG.maxRetries}):`, error.message);
                
                if (retryCount >= CONFIG.maxRetries) {
                    failed += mappedBatch.length;
                    console.error(`ğŸ’¥ Partia ${batchNumber} ostatecznie nieudana po ${CONFIG.maxRetries} prÃ³bach`);
                } else {
                    await new Promise(resolve => setTimeout(resolve, CONFIG.delayBetweenBatches * retryCount));
                }
            }
        }
        
        await new Promise(resolve => setTimeout(resolve, CONFIG.delayBetweenBatches));
    }
    
    const totalTime = (Date.now() - startTime) / 1000 / 60;
    
    return { success: imported, failed, skipped, stationStats, totalTime };
}

// GÅ‚Ã³wna funkcja peÅ‚nego importu
async function mainFullImport() {
    try {
        const sqlFile = path.join(process.cwd(), 'deximstr_hydro.sql');
        
        console.log(`ğŸ“ Plik SQL: ${sqlFile}`);
        console.log(`âš™ï¸  Konfiguracja: batch=${CONFIG.batchSize}, retry=${CONFIG.maxRetries}, delay=${CONFIG.delayBetweenBatches}ms\n`);
        
        // 1. Åadujemy mapowanie historycznych stacji
        const historicalMapping = loadHistoricalMapping();
        
        // 2. Parsujemy peÅ‚ny plik SQL
        console.log('\nğŸ”„ ETAP 1: Parsowanie pliku SQL');
        const { data } = parseSQLFileFull(sqlFile);
        
        // 3. Sprawdzamy poÅ‚Ä…czenie z bazÄ…
        console.log('\nğŸ”„ ETAP 2: PoÅ‚Ä…czenie z bazÄ… danych');
        await prisma.$connect();
        console.log('âœ… PoÅ‚Ä…czono z Supabase');
        
        // 4. Zapisujemy stan przed importem
        const beforeStats = await prisma.measurement.count();
        const beforeHistoricalCount = await prisma.measurement.count({
            where: { source: 'historical' }
        });
        console.log(`ğŸ“Š Pomiary przed importem: ${beforeStats.toLocaleString()} (historyczne: ${beforeHistoricalCount.toLocaleString()})`);
        
        // 5. PEÅNY IMPORT
        console.log('\nğŸ”„ ETAP 3: PEÅNY IMPORT DANYCH');
        if (data.measurements) {
            const measurementResult = await importMeasurementsFull(data.measurements, historicalMapping);
            
            console.log(`\nğŸ‰ WYNIKI PEÅNEGO IMPORTU:`);
            console.log(`   - âœ… Sukces: ${measurementResult.success.toLocaleString()} rekordÃ³w`);
            console.log(`   - âŒ BÅ‚Ä™dy: ${measurementResult.failed.toLocaleString()} rekordÃ³w`);
            console.log(`   - âš ï¸  PominiÄ™te: ${measurementResult.skipped.toLocaleString()} rekordÃ³w`);
            console.log(`   - ğŸ­ Stacje z danymi: ${Object.keys(measurementResult.stationStats).length}`);
            console.log(`   - â±ï¸  Czas importu: ${measurementResult.totalTime.toFixed(1)} minut`);
            
            // Top 10 stacji z najwiÄ™kszÄ… liczbÄ… pomiarÃ³w
            if (Object.keys(measurementResult.stationStats).length > 0) {
                console.log('\nğŸ“Š Top 10 stacji z najwiÄ™kszÄ… liczbÄ… historycznych pomiarÃ³w:');
                const topStations = Object.entries(measurementResult.stationStats)
                    .sort(([,a], [,b]) => b - a)
                    .slice(0, 10);
                
                for (const [stationId, count] of topStations) {
                    const station = await prisma.station.findUnique({
                        where: { id: stationId },
                        select: { stationName: true, stationCode: true }
                    });
                    console.log(`   - ${station?.stationName || 'Unknown'} (${station?.stationCode || stationId}): ${count.toLocaleString()} pomiarÃ³w`);
                }
            }
        }
        
        // 6. Sprawdzamy stan po imporcie
        const afterStats = await prisma.measurement.count();
        const afterHistoricalCount = await prisma.measurement.count({
            where: { source: 'historical' }
        });
        
        console.log(`\nğŸ“Š FINALNE STATYSTYKI:`);
        console.log(`   - Pomiary po imporcie: ${afterStats.toLocaleString()} (+${(afterStats - beforeStats).toLocaleString()})`);
        console.log(`   - Historyczne pomiary: ${afterHistoricalCount.toLocaleString()}`);
        
        if (afterStats > beforeStats) {
            console.log('\nğŸ‰ğŸ‰ğŸ‰ PEÅNY IMPORT ZAKOÅƒCZONY SUKCESEM! ğŸ‰ğŸ‰ğŸ‰');
            console.log('âœ… Historyczne dane hydrologiczne zostaÅ‚y zaimportowane do bazy Supabase');
            
            // Sprawdzamy zakres dat
            const dateRange = await prisma.measurement.aggregate({
                where: { source: 'historical' },
                _min: { measurementTimestamp: true },
                _max: { measurementTimestamp: true }
            });
            
            if (dateRange._min.measurementTimestamp && dateRange._max.measurementTimestamp) {
                console.log(`ğŸ“… Zakres historycznych danych: ${dateRange._min.measurementTimestamp.toISOString().split('T')[0]} - ${dateRange._max.measurementTimestamp.toISOString().split('T')[0]}`);
            }
            
            console.log('\nğŸš€ SYSTEM GOTOWY DO UÅ»YCIA!');
            console.log('ğŸ“‹ MoÅ¼esz teraz:');
            console.log('1. ğŸŒ TestowaÄ‡ API endpoints z historycznymi danymi');
            console.log('2. ğŸ“Š UÅ¼ywaÄ‡ endpoint /api/stations/[id]/history');
            console.log('3. ğŸ“ˆ AnalizowaÄ‡ dÅ‚ugoterminowe trendy hydrologiczne');
            console.log('4. ğŸ” PorÃ³wnywaÄ‡ dane historyczne z bieÅ¼Ä…cymi');
            
        } else {
            console.log('\nâš ï¸  PEÅNY IMPORT - brak nowych danych');
            console.log('SprawdÅº logi powyÅ¼ej aby zdiagnozowaÄ‡ problem');
        }
        
    } catch (error) {
        console.error('âŒ BÅ‚Ä…d podczas peÅ‚nego importu:', error);
        console.error('Stack trace:', error.stack);
    } finally {
        await prisma.$disconnect();
    }
}

if (require.main === module) {
    mainFullImport();
}

module.exports = { parseSQLFileFull, mapMeasurementData, importMeasurementsFull }; 